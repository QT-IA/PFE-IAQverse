"""
Script d'entraÃ®nement de modÃ¨les ML pour la prÃ©diction de qualitÃ© d'air intÃ©rieur.

Ce script entraÃ®ne un modÃ¨le capable de prÃ©dire pour N'IMPORTE QUELLE salle/capteur.
Les modÃ¨les prÃ©disent la qualitÃ© de l'air 30 minutes Ã  l'avance :
- CO2 (ppm)
- PM2.5 (Âµg/mÂ³)
- TVOC (ppb)
- HumiditÃ© (%)

Fonctionnement :
1. EntraÃ®nement initial sur le dataset preprocessÃ© 
2. RÃ©entraÃ®nement automatique toutes les heures avec les nouvelles donnÃ©es de l'API
3. Sauvegarde automatique des modÃ¨les mis Ã  jour
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
import json
import joblib
from datetime import datetime, timedelta
import time
import requests

# BibliothÃ¨ques Machine Learning
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

# Configuration des logs pour afficher les informations de traitement
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION

# ParamÃ¨tres de prÃ©diction
LOOKBACK_MINUTES = 12  # Nombre de points d'historique (12 x 5min = 1 heure)
FORECAST_MINUTES = 6   # Points Ã  prÃ©dire dans le futur (6 x 5min = 30 minutes)

# Colonnes de donnÃ©es
COLONNES_FEATURES = ['co2', 'pm25', 'tvoc', 'temperature', 'humidity']
COLONNES_CIBLES = ['co2', 'pm25', 'tvoc', 'humidity']

# URL de l'API pour rÃ©cupÃ©rer les nouvelles donnÃ©es
API_BASE_URL = "http://localhost:8000"

# Endpoint pour rÃ©cupÃ©rer la configuration des capteurs actifs
API_SENSORS_CONFIG_URL = f"{API_BASE_URL}/api/sensors-config"

# Intervalle de rÃ©entraÃ®nement (en secondes)
INTERVALLE_REENTRAINEMENT = 3600  # 1 heure

# ============================================================================
# FONCTIONS DE CHARGEMENT DES DONNÃ‰ES

def charger_dataset_csv(chemin_csv):
    logger.info(f"Chargement du dataset: {chemin_csv}")
    df = pd.read_csv(chemin_csv)
    
    # Nettoyer les noms de colonnes (enlever les espaces et guillemets)
    df.columns = df.columns.str.strip().str.strip('"').str.strip()
    
    # Nettoyer toutes les valeurs (enlever guillemets et espaces)
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].astype(str).str.strip().str.strip('"').str.strip()
    
    # Convertir les colonnes numÃ©riques
    for col in ['co2', 'pm25', 'tvoc', 'temperature', 'humidity']:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Convertir le timestamp en format datetime
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Trier par ordre chronologique
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    logger.info(f"Dataset chargÃ©: {len(df):,} lignes")
    
    # VÃ©rifier les colonnes nÃ©cessaires
    if 'salle' in df.columns:
        logger.info(f"Salles: {df['salle'].unique().tolist()}")
    if 'capteur_id' in df.columns:
        logger.info(f"Capteurs: {df['capteur_id'].unique().tolist()}")
    
    return df


def recuperer_config_ml():
    """
    RÃ©cupÃ¨re la configuration ML depuis l'API.
    Cette configuration est extraite automatiquement de config.json (lieux.enseignes.pieces).
    
    Format attendu de la rÃ©ponse:
    {
        "sensors": [
            {"enseigne": "Maison", "salle": "Bureau", "capteur_id": "Bureau1", "piece_id": "piece_xxx"},
            {"enseigne": "Maison", "salle": "Salon", "capteur_id": "Salon1", "piece_id": "piece_yyy"},
            ...
        ]
    }
    
    Returns:
        Liste de dictionnaires avec les configurations de capteurs, ou liste vide si erreur
    """
    try:
        logger.info(f"RÃ©cupÃ©ration de la configuration ML depuis: {API_SENSORS_CONFIG_URL}")
        response = requests.get(API_SENSORS_CONFIG_URL, timeout=10)
        
        if response.status_code == 200:
            config = response.json()
            sensors = config.get("sensors", [])
            
            if sensors:
                logger.info(f"âœ“ Configuration ML rÃ©cupÃ©rÃ©e: {len(sensors)} capteur(s)")
                for sensor in sensors:
                    enseigne = sensor.get("enseigne", "?")
                    salle = sensor.get("salle", "?")
                    capteur_id = sensor.get("capteur_id", "?")
                    logger.info(f"  â†’ {enseigne}/{salle}/{capteur_id}")
                return sensors
            else:
                logger.warning("Configuration ML vide, aucun capteur configurÃ©")
                return []
        else:
            logger.warning(f"Erreur lors de la rÃ©cupÃ©ration de la config ML: code {response.status_code}")
            return []
            
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration de la configuration ML: {e}")
        return []

def recuperer_nouvelles_donnees_api(enseigne="Maison", salle="Bureau"):
    """
    RÃ©cupÃ¨re les nouvelles donnÃ©es depuis l'API via l'endpoint /iaq/window.
    RÃ©cupÃ¨re la derniÃ¨re heure de donnÃ©es avec agrÃ©gation 5min.
    
    Args:
        enseigne: Nom de l'enseigne (dÃ©faut: "Maison")
        salle: Nom de la salle (dÃ©faut: "Bureau")
    
    Returns:
        DataFrame pandas avec les nouvelles donnÃ©es, ou DataFrame vide si erreur
    """
    try:
        # Utiliser l'endpoint /iaq/window pour rÃ©cupÃ©rer la derniÃ¨re heure
        url = f"{API_BASE_URL}/iaq/window"
        params = {
            "enseigne": enseigne,
            "salle": salle,
            "hours": 1,  # DerniÃ¨re heure
            "step": "5min"  # AgrÃ©gation 5 minutes
        }
        
        logger.info(f"RÃ©cupÃ©ration des donnÃ©es depuis l'API: {url}")
        logger.info(f"ParamÃ¨tres: enseigne={enseigne}, salle={salle}, hours=1, step=5min")
        
        response = requests.get(url, params=params, timeout=10)
        
        if response.status_code == 200:
            donnees = response.json()
            if donnees:
                df = pd.DataFrame(donnees)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                
                # S'assurer que les colonnes nÃ©cessaires sont prÃ©sentes
                if 'enseigne' not in df.columns:
                    df['enseigne'] = enseigne
                if 'salle' not in df.columns:
                    df['salle'] = salle
                if 'capteur_id' not in df.columns:
                    df['capteur_id'] = f"{salle}1"  # Capteur par dÃ©faut
                
                logger.info(f"âœ“ {len(df):,} nouvelles lignes rÃ©cupÃ©rÃ©es depuis l'API")
                return df
            else:
                logger.info("Aucune nouvelle donnÃ©e disponible dans l'API")
                return pd.DataFrame()
        else:
            logger.warning(f"Erreur API: code {response.status_code}")
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des donnÃ©es API: {e}")
        return pd.DataFrame()

def recuperer_toutes_les_donnees_api():
    """
    RÃ©cupÃ¨re les donnÃ©es de tous les capteurs configurÃ©s via l'API.
    Utilise l'endpoint /api/ml-config pour obtenir la liste des capteurs actifs.
    
    Returns:
        DataFrame combinÃ© avec toutes les nouvelles donnÃ©es des capteurs configurÃ©s
    """
    # RÃ©cupÃ©rer la configuration des capteurs depuis l'API
    sensors_config = recuperer_config_ml()
    
    if not sensors_config:
        logger.warning("Aucun capteur configurÃ©, utilisation des paramÃ¨tres par dÃ©faut")
        return recuperer_nouvelles_donnees_api()
    
    logger.info(f"RÃ©cupÃ©ration des donnÃ©es pour {len(sensors_config)} capteur(s) configurÃ©(s)")
    
    dataframes = []
    
    # RÃ©cupÃ©rer les donnÃ©es pour chaque capteur configurÃ©
    for sensor in sensors_config:
        enseigne = sensor.get("enseigne", "Maison")
        salle = sensor.get("salle", "Bureau")
        # capteur_id sera ajoutÃ© automatiquement par recuperer_nouvelles_donnees_api
        
        df_capteur = recuperer_nouvelles_donnees_api(enseigne, salle)
        
        if not df_capteur.empty:
            # Ajouter le capteur_id depuis la config si disponible
            if "capteur_id" in sensor:
                df_capteur["capteur_id"] = sensor["capteur_id"]
            
            dataframes.append(df_capteur)
    
    if not dataframes:
        logger.warning("Aucune nouvelle donnÃ©e rÃ©cupÃ©rÃ©e depuis l'API")
        return pd.DataFrame()
    
    # Combiner tous les DataFrames
    df_combine = pd.concat(dataframes, ignore_index=True)
    df_combine = df_combine.sort_values('timestamp').reset_index(drop=True)
    
    logger.info(f"âœ“ Total: {len(df_combine):,} nouvelles lignes rÃ©cupÃ©rÃ©es")
    
    return df_combine

def combiner_datasets(df_initial, df_nouvelles_donnees):
    """
    Combine le dataset initial avec les nouvelles donnÃ©es de l'API.
    
    Args:
        df_initial: DataFrame du dataset preprocessÃ©
        df_nouvelles_donnees: DataFrame des nouvelles donnÃ©es de l'API
        
    Returns:
        DataFrame combinÃ© et nettoyÃ©
    """
    if df_nouvelles_donnees.empty:
        logger.info("Pas de nouvelles donnÃ©es Ã  combiner")
        return df_initial
    
    # Combiner les deux DataFrames
    df_combine = pd.concat([df_initial, df_nouvelles_donnees], ignore_index=True)
    
    # Trier par timestamp
    df_combine = df_combine.sort_values('timestamp').reset_index(drop=True)
    
    # Supprimer les doublons (mÃªme timestamp, mÃªme salle, mÃªme capteur)
    df_combine = df_combine.drop_duplicates(
        subset=['timestamp', 'salle', 'capteur_id'],
        keep='last'  # Garder la version la plus rÃ©cente
    )
    
    logger.info(f"Dataset combinÃ©: {len(df_combine):,} lignes au total")
    
    return df_combine


# ============================================================================
# FONCTIONS DE PRÃ‰PARATION DES FEATURES
# ============================================================================

def creer_encodeurs():
    return LabelEncoder(), LabelEncoder()

def creer_features_temporelles(dataframe):
    df = dataframe.copy()
    # Heure du jour (0-23)
    df['hour'] = df['timestamp'].dt.hour
    # Jour de la semaine (0=lundi, 6=dimanche)
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    # Weekend ou non (1=weekend, 0=semaine)
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    return df

def encoder_salles_et_capteurs(dataframe, encodeur_salle, encodeur_capteur, mode='fit'):
    df = dataframe.copy()
    if mode == 'fit':
        # EntraÃ®nement: crÃ©er les encodages
        df['salle_encoded'] = encodeur_salle.fit_transform(df['salle'].fillna('Unknown'))
        df['capteur_encoded'] = encodeur_capteur.fit_transform(df['capteur_id'].fillna('Unknown'))
    else:
        # PrÃ©diction: utiliser les encodages existants
        df['salle_encoded'] = df['salle'].apply(
            lambda x: encodeur_salle.transform([x])[0] if x in encodeur_salle.classes_ else -1
        )
        df['capteur_encoded'] = df['capteur_id'].apply(
            lambda x: encodeur_capteur.transform([x])[0] if x in encodeur_capteur.classes_ else -1
        )  
    return df

def creer_features_statistiques(dataframe):
    df = dataframe.copy()
    # Traiter chaque capteur sÃ©parÃ©ment pour calculer les features
    for (salle, capteur) in df[['salle', 'capteur_id']].drop_duplicates().values:
        masque = (df['salle'] == salle) & (df['capteur_id'] == capteur)
        df_capteur = df[masque].copy()
        
        # Pour chaque type de mesure
        for colonne in COLONNES_FEATURES:
            if colonne in df_capteur.columns:
                # Tendance: diffÃ©rence avec la mesure prÃ©cÃ©dente
                df.loc[masque, f'{colonne}_diff'] = df_capteur[colonne].diff()
                # Moyenne mobile sur 3 points (filtre les variations brusques)
                df.loc[masque, f'{colonne}_ma3'] = df_capteur[colonne].rolling(
                    window=3, 
                    min_periods=1
                ).mean()
    # Remplir les valeurs NaN possiblement crÃ©Ã©es par les calculs
    # bfill = back fill (remplir avec la valeur suivante)
    # ffill = forward fill (remplir avec la valeur prÃ©cÃ©dente)
    df = df.bfill().ffill()
    return df

def preparer_toutes_les_features(dataframe, encodeur_salle, encodeur_capteur, mode='fit'):
    """
    Applique toutes les transformations de features sur le dataset.
    Args:
        dataframe: DataFrame brut
        encodeur_salle: LabelEncoder pour les salles
        encodeur_capteur: LabelEncoder pour les capteurs
        mode: 'fit' pour entraÃ®nement, 'transform' pour prÃ©diction
    """
    logger.info("CrÃ©ation des features...")
    # 1. Features temporelles
    df = creer_features_temporelles(dataframe)
    # 2. Encoder salles et capteurs
    df = encoder_salles_et_capteurs(df, encodeur_salle, encodeur_capteur, mode)
    # 3. Features statistiques
    df = creer_features_statistiques(df)
    logger.info(f"âœ“ Features crÃ©Ã©es: {len(df.columns)} colonnes au total")
    return df


# ============================================================================
# FONCTIONS DE PRÃ‰PARATION DES SÃ‰QUENCES
# ============================================================================

def preparer_sequences_entrainement(dataframe):
    """
    PrÃ©pare les sÃ©quences X (features) et y (cibles) pour l'entraÃ®nement.
    
    Pour chaque point dans le temps :
    - X : moyenne des mesures sur les LOOKBACK_MINUTES derniers points
    - y : valeurs cibles FORECAST_MINUTES points dans le futur
    
    Args:
        dataframe: DataFrame avec toutes les features
        
    Returns:
        Tuple (X, y, scaler)
        - X: array numpy des features normalisÃ©es
        - y: array numpy des cibles
        - scaler: StandardScaler utilisÃ© pour normaliser X
    """
    
    df = dataframe.sort_values(['salle', 'capteur_id', 'timestamp']).reset_index(drop=True)
    # Colonnes Ã  utiliser comme features (exclure les colonnes non-numÃ©riques)
    colonnes_features = [
        col for col in df.columns 
        if col not in ['timestamp', 'enseigne', 'salle', 'capteur_id']
    ]
    liste_X = []
    liste_y = []
    # CrÃ©er les sÃ©quences pour chaque capteur
    for (salle, capteur_id) in df[['salle', 'capteur_id']].drop_duplicates().values:
        masque = (df['salle'] == salle) & (df['capteur_id'] == capteur_id)
        df_capteur = df[masque].copy().reset_index(drop=True)
        
        # Pour chaque point temporel possible
        for i in range(len(df_capteur) - FORECAST_MINUTES):
            # DÃ©finir la fenÃªtre d'historique
            debut = max(0, i - LOOKBACK_MINUTES)
            
            # Extraire les features de la fenÃªtre
            fenetre = df_capteur.iloc[debut:i+1][colonnes_features].values
            
            # Calculer la moyenne de la fenÃªtre (rÃ©sume l'historique en un vecteur)
            if len(fenetre) > 0:
                features_moyennes = np.mean(fenetre, axis=0)
            else:
                features_moyennes = df_capteur.iloc[i][colonnes_features].values
            
            # Extraire les valeurs cibles dans le futur
            index_futur = i + FORECAST_MINUTES
            if index_futur < len(df_capteur):
                valeurs_cibles = df_capteur.iloc[index_futur][COLONNES_CIBLES].values
                liste_X.append(features_moyennes)
                liste_y.append(valeurs_cibles)
    
    # Convertir en arrays numpy
    X = np.array(liste_X)
    y = np.array(liste_y)
    
    # Normaliser les features (moyenne=0, Ã©cart-type=1)
    # La normalisation aide les modÃ¨les Ã  converger plus vite
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    logger.info(f"SÃ©quences crÃ©Ã©es:")
    logger.info(f"X: {X.shape[0]:,} exemples, {X.shape[1]} features")
    logger.info(f"y: {y.shape[0]:,} exemples, {y.shape[1]} cibles")
    
    return X, y, scaler


# ============================================================================
# FONCTIONS D'ENTRAÃŽNEMENT DES MODÃˆLES
# ============================================================================

def entrainer_modele_multi_output(X_train, y_train, X_val, y_val):
    """
    EntraÃ®ne UN SEUL VotingRegressor pour TOUTES les cibles simultanÃ©ment.
    
    Le VotingRegressor avec MultiOutputRegressor gÃ¨re automatiquement
    les prÃ©dictions multi-cibles en entraÃ®nant un modÃ¨le par cible en interne.
    
    Args:
        X_train: Features d'entraÃ®nement
        y_train: Toutes les cibles d'entraÃ®nement (4 colonnes: CO2, PM2.5, TVOC, humidity)
        X_val: Features de validation
        y_val: Toutes les cibles de validation
        
    Returns:
        Dictionnaire avec le modÃ¨le et les mÃ©triques par cible
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"EntraÃ®nement du modÃ¨le Ensemble Multi-Output")
    logger.info(f"PrÃ©diction simultanÃ©e de: {', '.join(COLONNES_CIBLES)}")
    logger.info(f"{'='*60}")
    
    # CrÃ©er les estimateurs de base (non entraÃ®nÃ©s)
    rf_base = RandomForestRegressor(
        n_estimators=100,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
    
    gb_base = GradientBoostingRegressor(
        n_estimators=100,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42
    )
    
    # CrÃ©er le VotingRegressor
    voting_base = VotingRegressor(
        estimators=[
            ('rf', rf_base),
            ('gb', gb_base)
        ],
        n_jobs=-1
    )
    
    # Envelopper dans MultiOutputRegressor pour gÃ©rer le multi-output
    logger.info("â†’ EntraÃ®nement Multi-Output Voting Regressor (RF + GB)...")
    
    modele_multi_output = MultiOutputRegressor(voting_base, n_jobs=-1)
    
    # EntraÃ®ner sur toutes les cibles en une seule fois
    modele_multi_output.fit(X_train, y_train)
    
    # Ã‰valuer le modÃ¨le sur l'ensemble de validation
    predictions = modele_multi_output.predict(X_val)
    
    # Calculer les mÃ©triques pour chaque cible
    logger.info("\nðŸ“Š MÃ©triques par cible:")
    
    metriques_par_cible = {}
    
    for idx, nom_cible in enumerate(COLONNES_CIBLES):
        y_val_cible = y_val[:, idx]
        pred_cible = predictions[:, idx]
        
        mse = mean_squared_error(y_val_cible, pred_cible)
        mae = mean_absolute_error(y_val_cible, pred_cible)
        r2 = r2_score(y_val_cible, pred_cible)
        
        metriques_par_cible[nom_cible] = {
            'mse': mse,
            'mae': mae,
            'r2': r2
        }
        
        logger.info(f"  â€¢ {nom_cible:12s} - RÂ²: {r2:.3f}, MAE: {mae:6.2f}, MSE: {mse:8.2f}")
    
    # Calculer les mÃ©triques globales (moyenne)
    mse_global = mean_squared_error(y_val, predictions)
    mae_global = mean_absolute_error(y_val, predictions)
    r2_global = r2_score(y_val, predictions)
    
    logger.info(f"\nðŸŽ¯ MÃ©triques globales (moyenne):")
    logger.info(f"   RÂ²: {r2_global:.3f}, MAE: {mae_global:.2f}, MSE: {mse_global:.2f}")
    
    # Retourner le modÃ¨le unique avec les mÃ©triques
    return {
        "model": modele_multi_output,
        "model_type": "voting_multi_output",
        "metrics_by_target": metriques_par_cible,
        "metrics_global": {
            'mse': mse_global,
            'mae': mae_global,
            'r2': r2_global
        }
    }


def entrainer_tous_les_modeles(X_train, y_train, X_val, y_val):
    """
    EntraÃ®ne UN SEUL modÃ¨le pour toutes les cibles.
    
    Args:
        X_train, y_train: DonnÃ©es d'entraÃ®nement
        X_val, y_val: DonnÃ©es de validation
        
    Returns:
        Dictionnaire avec le modÃ¨le unique et toutes les mÃ©triques
    """
    # EntraÃ®ner un seul modÃ¨le multi-output
    return entrainer_modele_multi_output(X_train, y_train, X_val, y_val)


# ============================================================================
# FONCTIONS DE SAUVEGARDE
# ============================================================================

def sauvegarder_modeles(modeles, scaler, encodeur_salle, encodeur_capteur, dossier_modeles):
    """
    Sauvegarde le modÃ¨le multi-output et objets nÃ©cessaires pour la prÃ©diction.
    
    Args:
        modeles: Dictionnaire contenant le modÃ¨le multi-output et ses mÃ©triques
                 Format: {"model": VotingRegressor, "model_type": str, 
                         "metrics_by_target": dict, "metrics_global": dict}
        scaler: StandardScaler pour normaliser les features
        encodeur_salle: LabelEncoder pour les salles
        encodeur_capteur: LabelEncoder pour les capteurs
        dossier_modeles: Dossier oÃ¹ sauvegarder les fichiers
    """
    # CrÃ©er le dossier s'il n'existe pas
    dossier_modeles.mkdir(parents=True, exist_ok=True)
    
    logger.info("\nSauvegarde du modÃ¨le multi-output...")
    
    # Sauvegarder le modÃ¨le multi-output unique
    modele = modeles["model"]
    model_type = modeles["model_type"]
    chemin_modele = dossier_modeles / "generic_multi_output.joblib"
    joblib.dump(modele, chemin_modele)
    logger.info(f"  âœ“ {chemin_modele.name} ({model_type})")
    
    # Sauvegarder le scaler
    chemin_scaler = dossier_modeles / "generic_scaler.joblib"
    joblib.dump(scaler, chemin_scaler)
    logger.info(f"  âœ“ {chemin_scaler.name}")
    
    # Sauvegarder les encodeurs
    chemin_salle = dossier_modeles / "salle_encoder.joblib"
    joblib.dump(encodeur_salle, chemin_salle)
    logger.info(f"  âœ“ {chemin_salle.name}")
    
    chemin_capteur = dossier_modeles / "capteur_encoder.joblib"
    joblib.dump(encodeur_capteur, chemin_capteur)
    logger.info(f"  âœ“ {chemin_capteur.name}")
    
    # Sauvegarder la configuration avec les mÃ©triques par cible
    config = {
        "lookback_minutes": LOOKBACK_MINUTES,
        "forecast_minutes": FORECAST_MINUTES,
        "feature_columns": COLONNES_FEATURES,
        "target_columns": COLONNES_CIBLES,
        "model_type": model_type,
        "metrics_by_target": modeles["metrics_by_target"],  # MÃ©triques de chaque cible
        "metrics_global": modeles["metrics_global"],  # MÃ©triques moyennes globales
        "salles_trained": list(encodeur_salle.classes_),
        "capteurs_trained": list(encodeur_capteur.classes_),
        "training_date": datetime.now().isoformat(),
        "version": "3.0"  # Version multi-output
    }
    
    chemin_config = dossier_modeles / "generic_training_config.json"
    with open(chemin_config, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    logger.info(f"  âœ“ {chemin_config.name}")
    
    # Afficher un rÃ©sumÃ© des mÃ©triques par cible
    logger.info("\nRÃ©sumÃ© des performances par cible:")
    metrics_by_target = modeles["metrics_by_target"]
    for target in COLONNES_CIBLES:
        metrics = metrics_by_target[target]
        r2 = metrics.get('r2', 0)
        mae = metrics.get('mae', 0)
        logger.info(f"  â€¢ {target}: {model_type.upper()} (RÂ²={r2:.3f}, MAE={mae:.2f})")


# ============================================================================
# PIPELINE COMPLET D'ENTRAÃŽNEMENT
# ============================================================================

def entrainer_modeles_complet(chemin_dataset, dossier_modeles, inclure_api=False):
    """
    Pipeline complet d'entraÃ®nement des modÃ¨les.
    
    Cette fonction orchestre tout le processus :
    1. Charge les donnÃ©es (CSV + optionnellement API)
    2. PrÃ©pare les features
    3. CrÃ©e les sÃ©quences d'entraÃ®nement
    4. EntraÃ®ne les modÃ¨les
    5. Sauvegarde tout
    
    Args:
        chemin_dataset: Chemin vers le CSV preprocessÃ©
        dossier_modeles: Dossier oÃ¹ sauvegarder les modÃ¨les
        inclure_api: Si True, combine avec les donnÃ©es de l'API
        
    Returns:
        True si succÃ¨s, False sinon
    """
    logger.info("\n" + "="*70)
    logger.info("ENTRAÃŽNEMENT DES MODÃˆLES ML - QUALITÃ‰ AIR INTÃ‰RIEUR")
    logger.info("="*70)
    
    try:
        # ===== 1. Charger les donnÃ©es =====
        df = charger_dataset_csv(chemin_dataset)
        
        # Ajouter les donnÃ©es de l'API si demandÃ©
        if inclure_api:
            df_api = recuperer_toutes_les_donnees_api()
            df = combiner_datasets(df, df_api)
        
        if df.empty or len(df) < 100:
            logger.error("Pas assez de donnÃ©es pour l'entraÃ®nement (minimum 100 lignes)")
            return False
        
        # ===== 2. PrÃ©parer les features =====
        encodeur_salle, encodeur_capteur = creer_encodeurs()
        df_features = preparer_toutes_les_features(df, encodeur_salle, encodeur_capteur, mode='fit')
        
        # ===== 3. CrÃ©er les sÃ©quences =====
        X, y, scaler = preparer_sequences_entrainement(df_features)
        
        if len(X) == 0:
            logger.error("Aucune sÃ©quence crÃ©Ã©e - donnÃ©es insuffisantes")
            return False
        
        # ===== 4. Diviser en train/validation (80/20) =====
        # shuffle=False pour respecter l'ordre chronologique
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        logger.info(f"\nDivision des donnÃ©es:")
        logger.info(f"  â€¢ EntraÃ®nement: {len(X_train):,} exemples")
        logger.info(f"  â€¢ Validation: {len(X_val):,} exemples")
        
        # ===== 5. EntraÃ®ner les modÃ¨les =====
        modeles = entrainer_tous_les_modeles(X_train, y_train, X_val, y_val)
        
        # ===== 6. Sauvegarder tout =====
        sauvegarder_modeles(modeles, scaler, encodeur_salle, encodeur_capteur, dossier_modeles)
        
        logger.info("\n" + "="*70)
        logger.info("âœ… ENTRAÃŽNEMENT TERMINÃ‰ AVEC SUCCÃˆS!")
        logger.info("="*70)
        logger.info(f"ModÃ¨les sauvegardÃ©s dans: {dossier_modeles}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erreur pendant l'entraÃ®nement: {e}", exc_info=True)
        return False


# ============================================================================
# RÃ‰ENTRAÃŽNEMENT AUTOMATIQUE
# ============================================================================

def boucle_reentrainement_automatique(chemin_dataset, dossier_modeles):
    """
    Boucle infinie qui rÃ©entraÃ®ne les modÃ¨les toutes les heures.
    
    Cette fonction :
    1. Attend INTERVALLE_REENTRAINEMENT secondes
    2. RÃ©cupÃ¨re les nouvelles donnÃ©es de l'API
    3. RÃ©entraÃ®ne les modÃ¨les
    4. Recommence
    
    Args:
        chemin_dataset: Chemin vers le CSV de base
        dossier_modeles: Dossier des modÃ¨les
    """
    logger.info("\n" + "="*70)
    logger.info("MODE RÃ‰ENTRAÃŽNEMENT AUTOMATIQUE ACTIVÃ‰")
    logger.info("="*70)
    logger.info(f"Intervalle: {INTERVALLE_REENTRAINEMENT} secondes ({INTERVALLE_REENTRAINEMENT/3600:.1f}h)")
    
    compteur = 1
    
    while True:
        try:
            # Attendre l'intervalle
            logger.info(f"\nâ° Prochaine mise Ã  jour dans {INTERVALLE_REENTRAINEMENT/60:.0f} minutes...")
            time.sleep(INTERVALLE_REENTRAINEMENT)
            
            logger.info(f"\n{'='*70}")
            logger.info(f"RÃ‰ENTRAÃŽNEMENT #{compteur}")
            logger.info(f"{'='*70}")
            
            # RÃ©entraÃ®ner avec les donnÃ©es de l'API
            succes = entrainer_modeles_complet(
                chemin_dataset, 
                dossier_modeles, 
                inclure_api=True
            )
            
            if succes:
                logger.info(f"âœ… RÃ©entraÃ®nement #{compteur} rÃ©ussi!")
                compteur += 1
            else:
                logger.warning(f"âš ï¸  RÃ©entraÃ®nement #{compteur} Ã©chouÃ©, nouvelle tentative dans {INTERVALLE_REENTRAINEMENT/60:.0f} min")
            
        except KeyboardInterrupt:
            logger.info("\n\nâ¹ï¸  ArrÃªt du rÃ©entraÃ®nement automatique demandÃ©")
            break
        except Exception as e:
            logger.error(f"âŒ Erreur pendant le rÃ©entraÃ®nement: {e}", exc_info=True)
            logger.info(f"Nouvelle tentative dans {INTERVALLE_REENTRAINEMENT/60:.0f} minutes...")


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """
    Point d'entrÃ©e principal du script.
    
    ExÃ©cute:
    1. EntraÃ®nement initial sur le dataset preprocessÃ©
    2. Lance le rÃ©entraÃ®nement automatique toutes les heures
    """
    # DÃ©finir les chemins
    dossier_base = Path(__file__).parent.parent
    chemin_dataset = dossier_base / "assets" / "datasets" / "ml_data" / "dataset_ml_5min.csv"
    dossier_modeles = dossier_base / "assets" / "ml_models"
    
    # VÃ©rifier que le dataset existe
    if not chemin_dataset.exists():
        logger.error(f"âŒ Dataset non trouvÃ©: {chemin_dataset}")
        logger.error("âš ï¸  Veuillez d'abord exÃ©cuter preprocess_dataset.py")
        return
    
    # ===== EntraÃ®nement initial =====
    logger.info("ðŸš€ DÃ©marrage de l'entraÃ®nement initial...")
    succes = entrainer_modeles_complet(chemin_dataset, dossier_modeles, inclure_api=False)
    
    if not succes:
        logger.error("âŒ EntraÃ®nement initial Ã©chouÃ© - arrÃªt du programme")
        return
    
    # ===== Lancer le rÃ©entraÃ®nement automatique =====
    logger.info("\nðŸ”„ Activation du rÃ©entraÃ®nement automatique...")
    logger.info("ðŸ’¡ Appuyez sur Ctrl+C pour arrÃªter")
    
    try:
        boucle_reentrainement_automatique(chemin_dataset, dossier_modeles)
    except KeyboardInterrupt:
        logger.info("\n\nðŸ‘‹ Programme terminÃ©")


if __name__ == "__main__":
    main()
